import requests
import pandas as pd
import yfinance as yf
import json
import os
import re
from datetime import datetime, date

# --- è¨­å®šå€ ---
TG_TOKEN = os.environ.get("TG_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'Referer': 'https://www.tpex.org.tw/web/bulletin/disposal_information/disposal_information.php?l=zh-tw'
}

def send_tg(message):
    if not TG_TOKEN or not TG_CHAT_ID: return
    try:
        url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
        requests.post(url, json={"chat_id": TG_CHAT_ID, "text": message, "parse_mode": "Markdown"})
    except: pass

def get_price(code, market):
    if not code or not str(code).isdigit() or len(str(code)) != 4:
        return "N/A", "N/A"
    suffix = ".TW" if market == "ä¸Šå¸‚" else ".TWO"
    try:
        ticker = yf.Ticker(f"{code}{suffix}")
        hist = ticker.history(period="1d", timeout=5)
        if hist.empty: return "N/A", "N/A"
        
        close = round(hist['Close'].iloc[-1], 2)
        prev = ticker.info.get('previousClose', None)
        if prev is None and len(hist['Open']) > 0: prev = hist['Open'].iloc[0]
        
        if prev:
            change = round(((close - prev) / prev) * 100, 2)
            return close, change
        return close, "N/A"
    except: return "N/A", "N/A"

def clean_html(raw_html):
    """æ¸…é™¤ HTML æ¨™ç±¤"""
    return re.sub(re.compile('<[^<]+?>'), '', str(raw_html)).strip()

def calc_countdown(period_str):
    """
    æš´åŠ›è§£ææ—¥æœŸï¼šè™•ç† 115/01/20~115/02/02 æˆ– - æˆ– ï½
    """
    try:
        # çµ±ä¸€åˆ†éš”ç¬¦è™Ÿï¼ŒæŠŠå„ç¨®æ³¢æµªè™Ÿã€å…¨å½¢åŠå½¢éƒ½æ›æˆç©ºæ ¼
        clean_str = str(period_str).replace('ï½', ' ').replace('~', ' ').replace('-', ' ')
        
        # æŠ“å–æ‰€æœ‰æ—¥æœŸ (æ°‘åœ‹å¹´ 3ç¢¼/2ç¢¼/2ç¢¼)
        matches = re.findall(r'(\d{3})[/](\d{2})[/](\d{2})', clean_str)
        
        if matches:
            # å–æœ€å¾Œä¸€çµ„ (çµæŸæ—¥)
            y_str, m_str, d_str = matches[-1]
            y = int(y_str)
            y = y + 1911 if y < 1911 else y
            
            target = date(y, int(m_str), int(d_str))
            diff = (target - date.today()).days
            return diff if diff >= 0 else 0
    except: return 0
    return 0

def parse_row_blindly(row_list, market_name):
    """
    ã€æ ¸å¿ƒé‚è¼¯ã€‘ç›²æœï¼šä¸ä¾è³´æ¬„ä½é †åºï¼Œæƒææ•´è¡Œè³‡æ–™æ‰¾ç‰¹å¾µ
    """
    item = {
        "market": market_name,
        "code": "",
        "name": "",
        "period": "",
        "reason": "",
        "level": "5åˆ†ç›¤",
        "end_date": "",
        "publish_date": ""
    }
    
    full_text = ""
    
    # ç¬¬ä¸€æ¬¡æƒæï¼šæ‰¾ä»£è™Ÿã€æ—¥æœŸã€é—œéµå­—
    for cell in row_list:
        txt = clean_html(cell)
        full_text += txt + " "
        
        # 1. æ‰¾ä»£è™Ÿ (4ä½æ•¸å­—) - å¦‚æœé‚„æ²’æ‰¾åˆ°
        if not item['code'] and re.match(r'^\d{4}$', txt):
            item['code'] = txt
            continue
            
        # 2. æ‰¾è™•ç½®æœŸé–“ (ç‰¹å¾µï¼šæœ‰æ—¥æœŸä¸”æœ‰æ³¢æµªè™Ÿæˆ–å…©å€‹æ—¥æœŸ)
        # æ ¼å¼å¦‚: 115/01/20~115/02/02
        dates = re.findall(r'\d{3}/\d{2}/\d{2}', txt)
        if not item['period'] and len(dates) >= 2:
            item['period'] = txt
            item['end_date'] = txt
            continue
            
        # 3. æ‰¾å…¬å¸ƒæ—¥æœŸ (ç‰¹å¾µï¼šå–®ä¸€å€‹æ—¥æœŸï¼Œä¸”ä¸æ˜¯è™•ç½®æœŸé–“)
        if not item['publish_date'] and len(dates) == 1 and len(txt) < 12:
             item['publish_date'] = txt
             continue

        # 4. æ‰¾åç¨± (éæ•¸å­—ã€é•·åº¦çŸ­ã€ä¸æ˜¯æ—¥æœŸ)
        if not item['name'] and not re.search(r'\d', txt) and len(txt) > 1 and len(txt) < 10:
            if "æª¢è¦–" not in txt and "è™•ç½®" not in txt:
                item['name'] = txt

    # åˆ¤æ–·åˆ†ç›¤ (å…¨æ–‡å­—æœå°‹)
    if "20åˆ†é˜" in full_text or "äºŒååˆ†é˜" in full_text:
        item['level'] = "20åˆ†ç›¤"
    elif "45åˆ†é˜" in full_text:
        item['level'] = "45åˆ†ç›¤"
    elif "60åˆ†é˜" in full_text:
        item['level'] = "60åˆ†ç›¤"
    elif "ç¬¬äºŒæ¬¡" in full_text:
        item['level'] = "20åˆ†ç›¤"

    # å¦‚æœæ²’æŠ“åˆ°æœŸé–“ï¼Œå†è©¦ä¸€æ¬¡æš´åŠ›æœå°‹
    if not item['period']:
        all_dates = re.findall(r'\d{3}/\d{2}/\d{2}', full_text)
        if len(all_dates) >= 2:
            # å–æœ€å¾Œå…©å€‹ç•¶ä½œå€é–“
            item['period'] = f"{all_dates[-2]}~{all_dates[-1]}"
            item['end_date'] = item['period']

    return item

def scrape_current():
    data = []
    
    # --- 1. ä¸Šå¸‚ (TWSE) ---
    print("æ­£åœ¨æŠ“å–ä¸Šå¸‚è³‡æ–™...")
    try:
        twse_headers = HEADERS.copy()
        twse_headers.pop('Referer', None)
        res = requests.get("https://www.twse.com.tw/rwd/zh/announcement/punish?response=json", headers=twse_headers, timeout=15)
        js = res.json()
        if js['stat'] == 'OK':
            print(f"ä¸Šå¸‚æŠ“åˆ° {len(js['data'])} ç­†")
            for r in js['data']:
                try:
                    # ç›²æœä¸Šå¸‚è³‡æ–™
                    parsed = parse_row_blindly(r, "ä¸Šå¸‚")
                    if parsed['code']:
                        data.append(parsed)
                except: continue
    except Exception as e: print(f"ä¸Šå¸‚éŒ¯èª¤: {e}")

    # --- 2. ä¸Šæ«ƒ (TPEx) ---
    print("æ­£åœ¨æŠ“å–ä¸Šæ«ƒè³‡æ–™...")
    try:
        url = "https://www.tpex.org.tw/web/bulletin/disposal_information/disposal_information_result.php?l=zh-tw&o=json"
        res = requests.get(url, headers=HEADERS, timeout=15)
        js = res.json()
        rows = js.get('aaData', [])
        print(f"ä¸Šæ«ƒæŠ“åˆ° {len(rows)} ç­†")
        
        for r in rows:
            try:
                # ç›²æœä¸Šæ«ƒè³‡æ–™
                parsed = parse_row_blindly(r, "ä¸Šæ«ƒ")
                if parsed['code']:
                    data.append(parsed)
            except: continue
            
    except Exception as e: print(f"ä¸Šæ«ƒéŒ¯èª¤: {e}")

    return data

def main():
    print("=== ç¨‹å¼é–‹å§‹åŸ·è¡Œ ===")
    
    old_data = {"disposal_stocks": [], "exited_stocks": []}
    if os.path.exists('data.json'):
        try:
            with open('data.json', 'r', encoding='utf-8') as f:
                old_data = json.load(f)
        except: pass
    
    # æŠ“å–æ–°è³‡æ–™
    raw_new = scrape_current()
    
    new_processed = []
    new_codes = set()
    tg_msg_list = []

    # è™•ç†æ–°è³‡æ–™
    for s in raw_new:
        code = s['code']
        new_codes.add(code)
        
        # é€™è£¡ä¸æ¯”å°èˆŠè³‡æ–™ï¼Œç›´æ¥è¦–ç‚ºæœ€æ–°ç‹€æ…‹
        # å› ç‚ºèˆŠè³‡æ–™å¯èƒ½å£äº†
        price, change = get_price(code, s['market'])
        countdown = calc_countdown(s['end_date'])
        
        new_processed.append({
            **s, "price": price, "change": change, "countdown": countdown
        })

    # æ’åº
    new_processed.sort(key=lambda x: x['countdown'])

    # --- è™•ç†å‡ºé—œèˆ‡å¾©æ´» ---
    recently_exited = []
    
    # 1. æª¢æŸ¥èˆŠçš„è™•ç½®è‚¡ (å¦‚æœæ–°åå–®æ²’æœ‰ï¼Œæ‰ç®—èˆ‡å‡ºé—œ)
    for old_s in old_data.get('disposal_stocks', []):
        if old_s['code'] not in new_codes:
            # çœŸçš„æ¶ˆå¤±äº†ï¼ŒåŠ å…¥å‡ºé—œæ¸…å–®
            p, c = get_price(old_s['code'], old_s['market'])
            old_s.update({"price": p, "change": c, "exit_date": datetime.now().strftime("%Y-%m-%d")})
            recently_exited.append(old_s)

    # 2. æª¢æŸ¥åŸæœ¬åœ¨ã€Œå‰›å‡ºé—œã€æ¸…å–®è£¡çš„
    for ex in old_data.get('exited_stocks', []):
        try:
            # ã€å¾©æ´»æ©Ÿåˆ¶ã€‘å¦‚æœé€™å€‹è‚¡ç¥¨å‡ºç¾åœ¨æ–°æŠ“åˆ°çš„åå–®(new_codes)è£¡
            # ä»£è¡¨å®ƒä¹‹å‰è¢«èª¤åˆ¤å‡ºé—œäº†ï¼Œç¾åœ¨è¦å¿½ç•¥å®ƒ(è®“å®ƒç•™åœ¨ disposal_stocks)
            if ex['code'] in new_codes:
                continue

            # æ­£å¸¸çš„å‡ºé—œé‚è¼¯
            days_diff = (datetime.now() - datetime.strptime(ex['exit_date'], "%Y-%m-%d")).days
            if days_diff <= 5:
                if ex['code'] not in [x['code'] for x in recently_exited]:
                    recently_exited.append(ex)
        except: pass

    # ç”¢ç”Ÿé€šçŸ¥ (åªé€šçŸ¥çœŸçš„æ–°å‡ºç¾çš„)
    # è®€å–èˆŠçš„ valid codes ä¾†æ¯”å°ï¼Œé¿å…é‡è¤‡é€šçŸ¥
    old_valid_codes = {s['code'] for s in old_data.get('disposal_stocks', [])}
    for s in new_processed:
        if s['code'] not in old_valid_codes and len(old_valid_codes) > 0:
            tg_msg_list.append(s)

    etf_data = [
        {"code":"00940","name":"å…ƒå¤§è‡ºç£åƒ¹å€¼é«˜æ¯","action":"æ–°å¢","stock":"é•·æ¦®èˆª(2618)","date":"2026-05-17"},
        {"code":"00878","name":"åœ‹æ³°æ°¸çºŒé«˜è‚¡æ¯","action":"åˆªé™¤","stock":"è‹±æ¥­é”(2356)","date":"2026-05-20"}
    ]

    if tg_msg_list:
        msg_lines = ["ğŸš¨ **å°è‚¡è™•ç½®æ–°å¢**"]
        for x in tg_msg_list:
            pub = x.get('publish_date', 'æœªçŸ¥')
            msg_lines.append(f"{x['name']}({x['code']})\n{x['level']} | å…¬å¸ƒ: {pub}")
        send_tg("\n\n".join(msg_lines))

    final_output = {
        "update_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "disposal_stocks": new_processed,
        "exited_stocks": recently_exited,
        "etf_stocks": etf_data
    }
    
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=4)
        
    print(f"=== åŸ·è¡ŒçµæŸï¼ŒæˆåŠŸè™•ç† {len(new_processed)} ç­†è³‡æ–™ ===")

if __name__ == "__main__":
    main()
